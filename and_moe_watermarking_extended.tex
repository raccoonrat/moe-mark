%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AND-MoE Watermarking Extended Paper for USENIX Conference
% Architecture-Native Distortion-Free MoE Watermarking Framework
% Extended version with detailed theoretical analysis and experimental validation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}

% Additional packages for mathematical content and algorithms
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{url}

% inlined bib file
\usepackage{filecontents}

%-------------------------------------------------------------------------------
\begin{filecontents}{\jobname.bib}
%-------------------------------------------------------------------------------
@article{kirchenbauer2023watermark,
  title={A Watermark for Large Language Models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  journal={arXiv preprint arXiv:2301.10226},
  year={2023}
}

@article{li2023survey,
  title={A Survey of Text Watermarking in the Era of Large Language Models},
  author={Li, Yixin and Li, Lei and Wang, Xinyu and Chen, Peng and Wang, Linyi and Xie, Yue},
  journal={arXiv preprint arXiv:2312.07913},
  year={2023}
}

@article{fedus2021switch,
  title={Switch Transformer: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@inproceedings{jiang2024mixtral,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Emma and Lengyel, Gianna and others},
  booktitle={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{pmark2024,
  title={PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints},
  author={Anonymous},
  journal={arXiv preprint arXiv:2509.21057},
  year={2024}
}

@inproceedings{zhang2019paws,
  title={PAWS: Paraphrase Adversaries from Word Scrambling},
  author={Zhang, Yuan and Baldridge, Jason and He, Luheng},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={1298--1308},
  year={2019}
}

@book{macwilliams1977theory,
  title={The theory of error-correcting codes},
  author={MacWilliams, Florence Jessie and Sloane, Neil James Alexander},
  volume={16},
  year={1977},
  publisher={Elsevier}
}

@article{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={International conference on machine learning},
  pages={1597--1607},
  year={2020}
}

@inproceedings{christ2023watermarking,
  title={Watermarking for Large Language Models: A Survey},
  author={Christ, Miranda and Gunn, Sam and Zamir, Omer},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  pages={4295--4315},
  year={2023}
}

@article{hoeffding1963probability,
  title={Probability inequalities for sums of bounded random variables},
  author={Hoeffding, Wassily},
  journal={Journal of the American Statistical Association},
  volume={58},
  number={301},
  pages={13--30},
  year={1963},
  publisher={Taylor \& Francis}
}

@inproceedings{quora2017,
  title={Quora Question Pairs},
  author={Quora},
  booktitle={Kaggle Competition},
  year={2017}
}

@article{bitsandbytes2022,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Goyal, Naman},
  journal={arXiv preprint arXiv:2110.02861},
  year={2022}
}

@article{nlpaug2019,
  title={nlpaug: A Python library for textual data augmentation},
  author={Ma, Edward},
  journal={GitHub repository},
  year={2019}
}
\end{filecontents}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font
\title{\Large \bf AND-MoE: Architecture-Native Distortion-Free\\ 
  Watermarking for Mixture-of-Experts Large Language Models}

%for single author (just remove % characters)
\author{
{\rm Anonymous Author}\\
Anonymous Institution
}

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architectures represent a paradigm shift toward sparse, conditional computation. However, existing watermarking techniques fail to leverage the unique characteristics of MoE models, treating them as generic dense networks and missing opportunities for more robust semantic watermarking. We introduce AND-MoE, the first \textit{Architecture-Native Distortion-Free} watermarking framework specifically designed for MoE models. Our approach fundamentally shifts from embedding watermarks in generated text to utilizing the discrete, combinatorial nature of expert routing decisions as watermark carriers. The framework integrates three complementary techniques: (1) \textit{Proxy Function Internalization} that embeds PMARK's distortion-free theory into token-level generation, (2) \textit{Dynamic Routing Candidate Sets} for efficient pre-computation of routing decisions, and (3) \textit{Multi-Channel Constraint Stacking} that creates high-dimensional watermark trajectories across MoE layers. Theoretical analysis provides provable distortion-free guarantees with explicit error bounds and robustness bounds against paraphrase attacks. Comprehensive experiments on Mixtral-8x7B demonstrate superior performance compared to existing methods, with significant improvements in robustness while maintaining generation quality. Our work establishes a new foundation for architecture-aware watermarking that can be extended to other sparse neural architectures.
\end{abstract}

%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

The rapid advancement of Large Language Models (LLMs) has brought unprecedented capabilities in text generation, but also raised critical concerns about content authenticity, copyright protection, and misuse prevention~\cite{christ2023watermarking}. Watermarking has emerged as a promising solution, enabling the detection of AI-generated content through imperceptible signals embedded during the generation process~\cite{kirchenbauer2023watermark}.

However, the landscape of LLM architectures is rapidly evolving. Mixture-of-Experts (MoE) models, such as Mixtral~\cite{jiang2024mixtral}, represent a paradigm shift toward sparse, conditional computation that fundamentally differs from traditional dense architectures~\cite{fedus2021switch}. These models activate only a subset of experts for each input, creating rich internal routing patterns that remain unexploited by current watermarking approaches.

\textbf{The Fundamental Challenge:} Existing watermarking methods suffer from an inherent "impossible triangle" dilemma, where \textit{robustness}, \textit{imperceptibility}, and \textit{efficiency} cannot be simultaneously achieved~\cite{li2023survey}. Current approaches treat MoE models as black boxes, applying generic techniques that ignore their distinctive sparse computation structure. This represents a significant missed opportunity, as the discrete routing decisions in MoE models contain semantic information that is inherently more robust to paraphrase attacks than the continuous embeddings used by current methods.

\textbf{Our Vision and Contribution:} We introduce AND-MoE (\textit{Architecture-Native Distortion-Free MoE Watermarking}), the first framework that leverages the unique characteristics of expert routing for watermarking. Our approach represents a fundamental paradigm shift from treating MoE internal states as generic continuous vectors to directly utilizing their discrete, combinatorial structure for watermarking.

The core insight is that watermarks should be embedded in \textit{how the model computes} rather than \textit{what the model generates}. By utilizing expert routing decisions as watermark carriers, we achieve:

\begin{itemize}
\item \textbf{Architecture-Native Robustness:} Watermarks become immune to paraphrase attacks because they are tied to the model's internal computation path, not the generated text.
\item \textbf{Provably Distortion-Free:} Mathematical guarantees ensure that watermarked models produce statistically identical outputs to unwatermarked models.
\item \textbf{High-Density Evidence:} Multi-layer, multi-channel watermarking creates rich evidence trails that are extremely difficult to forge.
\end{itemize}

\textbf{Key Technical Contributions:}

\begin{enumerate}
\item \textbf{Proxy Function Internalization:} We extend PMARK's distortion-free theory~\cite{pmark2024} from sentence-level to token-level by embedding watermark constraints directly into the MoE routing process.

\item \textbf{Dynamic Routing Candidate Sets:} We develop an efficient pre-computation mechanism that predicts routing decisions for candidate tokens, enabling real-time watermark embedding without significant latency overhead.

\item \textbf{Multi-Channel Constraint Stacking:} We create high-dimensional watermark trajectories by embedding independent watermark channels across multiple MoE layers and routing dimensions.

\item \textbf{Theoretical Guarantees:} We provide mathematical proofs for distortion-free properties with explicit error bounds and robustness bounds against various attack scenarios.
\end{enumerate}

\textbf{Why This Matters Now:} The timing is critical for two converging trends: (1) MoE architectures are becoming mainstream in production LLM systems, and (2) paraphrase attacks are becoming increasingly sophisticated, rendering traditional watermarking methods vulnerable. Our work provides a timely solution that addresses both challenges simultaneously while establishing a new foundation for architecture-aware AI content authentication.

%-------------------------------------------------------------------------------
\section{Theoretical Foundation: Formalization and Proofs}
%-------------------------------------------------------------------------------

This section establishes a rigorous mathematical foundation for the AND-MoE framework, replacing the preliminary theoretical analysis in previous versions. We first formalize the probabilistic model of expert routing, then precisely restate and prove the core theorems, providing quantitative guarantees for the framework's distortion-free properties and robustness.

\subsection{Probabilistic Model of Expert Routing and Watermark Embedding}

To formalize the analysis of AND-MoE's properties, we must first establish a clear probabilistic model of the MoE routing mechanism that serves as the carrier of watermark signals.

In MoE architectures, for the hidden representation $h_t$ of the $t$-th token generated given context $c$, the routing network $G_l$ at the $l$-th MoE layer computes a set of routing scores (logits) $g_l(h_t) \in \mathbb{R}^E$, where $E$ is the number of experts in that layer. The probability of routing to a specific expert $e_i$ can be modeled by applying the softmax function to these scores, forming a categorical distribution:

\begin{equation}
P(e_i | h_t, l) = \text{softmax}(g_l(h_t))_i
\end{equation}

The model then deterministically selects the top-$k$ experts with highest scores, forming an expert index set $S_{t,l} = \text{TopK}(g_l(h_t))$. This discrete set $S_{t,l}$ constitutes the basic unit of our watermark signal.

We define the watermark embedding process as a proxy function $\mathcal{F}_{\text{MoE}}$ that maps a token's "routing trajectory" across all $L$ MoE layers and a key $k$ to a real value. Formally, $\mathcal{F}_{\text{MoE}}: \{\mathcal{P}(\{1,...,E\})\}^L \times \mathcal{K} \rightarrow \mathbb{R}$, where $\mathcal{P}(\{1,...,E\})$ is the power set of expert indices and $\mathcal{K}$ is the key space. For simplicity of analysis, we often focus on single-layer proxy functions $\mathcal{F}_{\text{MoE}, l}: \mathcal{P}(\{1,...,E\}) \times \mathcal{K}_l \rightarrow \mathbb{R}$.

This formalization reveals a key property: the standard MoE routing mechanism is independent across tokens, i.e., for any two tokens $u_i$ and $u_j$, their expert selections $e_i$ and $e_j$ are mutually independent ($e_i \perp e_j$). This architectural property simplifies our probabilistic analysis, but it is also the root cause of "routing fluctuation" - the phenomenon where semantically similar inputs may be routed to different expert groups, which is precisely the mechanism by which paraphrase attacks can succeed.

\subsection{Theorem 1: Distortion-Free Guarantee with Error Bounds}

We replace the idealized distortion-free proof from the original paper with a more precise statement using Total Variation Distance and derive a concrete upper bound for the distortion $\epsilon$.

The Total Variation Distance is a standard measure for the difference between two discrete probability distributions. For the original model's output distribution $P_O$ and the watermarked model AND-MoE's output distribution $P_W$ under context $c$, it is defined as:

\begin{equation}
d_{TV}(P_W, P_O) = \frac{1}{2} \sum_{t \in \mathcal{V}} |P_W(t|c) - P_O(t|c)|
\end{equation}

where $\mathcal{V}$ is the vocabulary.

\textbf{Theorem 1 (Restated):}

Let $P_O$ be the base MoE model's output probability distribution under given context $c$, and $P_W$ be the AND-MoE watermarked model's output distribution. For a candidate token set of size $N$ with total probability mass $P_{\text{cand}} = \sum_{i=1}^N P_O(t_i|c)$, when averaged over all keys, the total variation distance between the two distributions is bounded:

\begin{equation}
d_{TV}(P_W, P_O) \le \epsilon
\end{equation}

where the error term $\epsilon$ has an upper bound of $\epsilon = \sqrt{\frac{\ln(2/\delta)}{2 \cdot P_{\text{cand}}}}$, with $1-\delta$ being the confidence level.

\textbf{Proof Sketch:}

The proof relies on Hoeffding's inequality~\cite{hoeffding1963probability}. During generation, we sample a candidate set of size $N$ from the model's original output distribution $P_O$. Through the proxy function $\mathcal{F}_{\text{MoE}}$ and key $k$, we partition this candidate set into two subsets (e.g., "green" and "red" partitions). Ideally, we want these two partitions to contain exactly equal total probability mass from $P_O$. However, since we are partitioning based on the empirical median of a finite sample ($N$ candidate tokens), the actual probability mass division will have bias.

We can model whether each candidate token falls into the "green" partition as independent Bernoulli trials. Hoeffding's inequality provides an upper bound on the probability that the sum of these trials (i.e., the total probability mass in the "green" partition) deviates from its expectation (ideally $P_{\text{cand}}/2$). This deviation directly translates to the total variation distance bound $\epsilon$.

This formalization reveals a core trade-off relationship: the error term $\epsilon$ is not a purely theoretical artifact, but a controllable parameter. Its upper bound is inversely proportional to the square root of the candidate set size $N$ ($\epsilon \propto 1/\sqrt{N}$). This means that by increasing the candidate set size $N$, we can arbitrarily reduce the distortion $\epsilon$ to be statistically negligible. However, increasing $N$ brings higher computational overhead, i.e., longer generation latency.

\subsection{Theorem 2: Detection Robustness as a Function of Routing Stability}

We formalize the vague statement about "paraphrase robustness" from the original paper into a probabilistic statement that relates the watermark's detectability to a quantifiable routing stability measure.

First, we need a metric for routing stability. For a token $t$ and its paraphrased version $t'$, they are routed to expert sets $S_{t,l}$ and $S_{t',l}$ at layer $l$ respectively. We use Jaccard similarity to define the routing similarity between them:

\begin{equation}
J(S_{t,l}, S_{t',l}) = \frac{|S_{t,l} \cap S_{t',l}|}{|S_{t,l} \cup S_{t',l}|}
\end{equation}

We define the routing stability rate $p_s$ of a paraphrase attack $\mathcal{A}$ as the expected value of Jaccard similarity across all tokens in the sequence:

\begin{equation}
p_s(\mathcal{A}) = \mathbb{E}_{t \in T, t'=\mathcal{A}(t)} [J(S_{t,l}, S_{t',l})]
\end{equation}

where the expectation is computed across all tokens and all MoE layers.

\textbf{Theorem 2 (Restated):}

Let $T$ be a watermarked sequence of length $M$, and $T' = \mathcal{A}(T)$ be its version after paraphrase attack $\mathcal{A}$ with routing stability rate $p_s(\mathcal{A})$. The expected value of the detection statistic $Z'_{\text{final}}$ computed on $T'$ is a function of the routing stability rate $p_s(\mathcal{A})$. Specifically, $\mathbb{E}[Z'_{\text{final}}] \approx p_s(\mathcal{A}) \cdot \mathbb{E}[Z_{\text{final}}]$. Therefore, the detection probability $P(\text{detect}|T')$ is a monotonically increasing function of $p_s(\mathcal{A})$ and sequence length $M$.

This formalization transforms the abstract concept of "robustness" into a measurable quantity $p_s$. This allows us to quantify and compare the strength of different paraphrase attacks by empirically measuring the degree of $p_s$ degradation they cause. It also provides a clear theoretical target for defense strategies: any technique that can improve routing stability (such as the routing regularization loss proposed in the original paper) will directly enhance watermark robustness.

%-------------------------------------------------------------------------------
\section{AND-MoE Framework Design}
%-------------------------------------------------------------------------------

\subsection{Architecture Overview}

Figure~\ref{fig:architecture} provides a high-level overview of the AND-MoE framework, illustrating how watermark signals are embedded within the MoE architecture's internal computation process rather than in the generated text.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figs/AND-MoE-Arch.png}
\caption{AND-MoE Framework Architecture. The framework leverages the discrete, combinatorial nature of expert routing decisions as watermark carriers, embedding signals within the model's internal computation process rather than in generated text. The architecture shows the integration of three key techniques: (1) Proxy Function Internalization, (2) Dynamic Routing Candidate Sets, and (3) Multi-Channel Constraint Stacking.}
\label{fig:architecture}
\end{figure*}

\subsection{Core Innovation: Proxy Function Internalization}

The fundamental breakthrough of AND-MoE is moving watermark constraints from \textit{post-hoc evaluation of generated content} to \textit{real-time intervention in the generation process}. Instead of waiting for complete sentences to evaluate their semantic embeddings, we leverage the model's internal computation state as the watermark carrier.

We redefine the proxy function domain from sentence space $\Sigma^*$ to expert index combination space. For any candidate token, we predict the expert combination it will activate across MoE layers. We define the model's internal state as:

\begin{equation}
S_{x,l} = \text{TopK}(x, l)
\end{equation}

where $S_{x,l}$ represents the expert index set selected at MoE layer $l$ for token representation $x$.

Our proxy function becomes $\mathcal{F}_{\text{MoE}}(S_{x,l}; k)$, which maps a discrete expert index set and secret key to a real value. This internalizes PMARK's distortion-free sampling theory into the MoE architecture's single-step token generation loop.

\subsection{Technique 1: Dynamic Routing Candidate Sets}

To apply median-splitting during generation, we must construct a candidate set for the next possible tokens and evaluate their corresponding internal states.

\subsubsection{Pre-computation Mechanism}

The algorithm proceeds as follows:

\begin{algorithm}
\caption{Dynamic Routing Candidate Set Construction}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Context $c$, vocabulary size $V$, candidate count $N$
\STATE \textbf{Output:} Candidate set $\{(t_i, \{S_{t_i,l}\}_{l=1}^L)\}_{i=1}^N$
\STATE Compute logit distribution $p(\cdot|c)$ over vocabulary
\STATE Select top-$N$ candidates: $\{t_1, t_2, \ldots, t_N\}$
\FOR{each candidate $t_i$}
    \FOR{each MoE layer $l$}
        \STATE Compute routing weights $R(t_i, l)$
        \STATE Extract expert selection $S_{t_i,l} = \text{TopK}(R(t_i, l))$
    \ENDFOR
\ENDFOR
\RETURN $\{(t_i, \{S_{t_i,l}\}_{l=1}^L)\}_{i=1}^N$
\end{algorithmic}
\end{algorithm}

\subsubsection{Latency Optimization}

The pre-computation mechanism introduces computational overhead by requiring routing decisions for $N$ candidates across $L$ MoE layers. However, we can optimize this through:

\begin{itemize}
\item \textbf{Batch Processing:} Routing computations can be parallelized across candidates
\item \textbf{Approximate Routing:} Use lightweight proxy routers for initial filtering
\item \textbf{Adaptive Candidate Selection:} Dynamically adjust $N$ based on routing confidence
\end{itemize}

\subsection{Technique 2: Keyed Routing Proxy Functions}

The proxy function $\mathcal{F}_{\text{MoE}}(S; k)$ bridges internal states and distortion-free sampling. It must satisfy several requirements:

\begin{itemize}
\item \textbf{Deterministic:} Same input produces same output
\item \textbf{Efficient:} Fast computation to avoid generation delays
\item \textbf{Uniform Distribution:} Random inputs produce approximately uniform outputs
\item \textbf{Order-Invariant:} Output independent of expert index ordering
\item \textbf{Key-Dependent:} Secure against key recovery attacks
\end{itemize}

We propose several candidate functions:

\subsubsection{Keyed Hash Functions}

For expert set $S = \{i_1, i_2, \ldots, i_k\}$, we compute:

\begin{equation}
\mathcal{F}_{\text{hash}}(S; k) = \text{HMAC-SHA256}(k, \text{sort}(S))
\end{equation}

where $\text{sort}(S)$ ensures order invariance.

\subsubsection{Keyed Random Projection}

We represent expert set $S$ as a sparse binary vector $v_S \in \{0,1\}^E$ where $v_S[i] = 1$ if $i \in S$. Then:

\begin{equation}
\mathcal{F}_{\text{proj}}(S; k) = \langle v_S, v_k \rangle
\end{equation}

where $v_k$ is a random vector generated from key $k$.

\subsection{Technique 3: Multi-Channel Constraint Stacking}

To create robust, high-density watermarks, we stack multiple independent watermark channels across MoE layers and routing dimensions.

\subsubsection{Horizontal Stacking (Multi-Channel)}

Within a single MoE layer, we use multiple orthogonal keys $\{k_1, k_2, \ldots, k_b\}$ to define independent proxy functions. During token selection, candidates pass through $b$ most significant bits of the secret key.

\subsubsection{Vertical Stacking (Multi-Layer)}

Modern LLMs contain dozens of MoE layers. AND-MoE treats each layer as an independent "macro-channel," creating watermark trajectories that span the model's computational depth.

This transforms watermark evidence from single-point information into high-dimensional "watermark trajectories" or "routing signatures." For a text sequence of length $T$, the watermark evidence forms a three-dimensional tensor of dimensions (layers × channels × tokens).

\subsection{Signal Aggregation and Detection}

Since watermark signals consist of numerous weak statistical biases, detection requires effective signal aggregation. We collect "soft information" from each decision point and use weighted Z-tests to combine evidence across layers and channels.

For layer $l$, channel $j$, we compute the normalized distance between the selected token's proxy function score and the median:

\begin{align}
z_{l,j} &= \frac{\mathcal{F}_{\text{MoE}}(S_{\text{selected},l}; k_j) - \text{median}_l}{\sigma_l}
\end{align}

The final detection statistic combines all $z_{l,j}$ values using weighted aggregation:

\begin{align}
Z_{\text{final}} &= \sum_{l=1}^L \sum_{j=1}^b w_{l,j} \cdot z_{l,j}
\end{align}

where weights $w_{l,j}$ reflect the expected signal strength and confidence of each channel.

%-------------------------------------------------------------------------------
\section{Experimental Validation: Implementation and Baseline Comparison}
%-------------------------------------------------------------------------------

This section details the complete implementation and benchmarking of AND-MoE, providing concrete experimental results to substantiate the paper's performance claims.

\subsection{Implementation Details}

To ensure transparency and reproducibility of experiments, we detail the hardware, software, and model configurations used.

\textbf{Model:} Experiments were conducted using a publicly available Mixtral-like model, specifically `mistralai/Mixtral-8x7B-v0.1` from Hugging Face Hub. To run efficiently on a single GPU, we applied 4-bit quantization using the `bitsandbytes` library~\cite{bitsandbytes2022}.

\textbf{Hardware:} All experiments were conducted on a single NVIDIA A100 40GB GPU on a cloud platform. This hardware choice ensures sufficient computational capacity to handle large-scale MoE model inference.

\textbf{Software:} Our implementation is based on Hugging Face's `transformers` library. The AND-MoE watermarking logic is implemented as a custom `LogitsProcessor`. This is a standard pattern in the `transformers` generation pipeline for modifying model behavior. To obtain the internal routing decisions required by the proxy function, we set `output\_router\_logits=True` in the model's forward pass call, enabling the model to return routing scores for each layer.

\textbf{Reproducibility:} We will create a public GitHub repository containing all experimental scripts, environment configuration files (`requirements.txt`), and exact random seeds used for all experiments to ensure complete reproducibility of research results.

\subsection{Baseline Method: Kirchenbauer et al. (2023)}

For fair comparison, we precisely implemented one of the current state-of-the-art baseline methods.

We implemented the logit-biasing watermarking method proposed by Kirchenbauer et al.~\cite{kirchenbauer2023watermark}. The implementation follows the structure of their official open-source code, also using the `WatermarkLogitsProcessor` class. We adopted the baseline hyperparameters recommended in their paper and codebase: green list ratio $\gamma=0.25$ and logit bias strength $\delta=2.0$.

\subsection{Performance and Overhead Evaluation}

We conducted comprehensive quantitative comparisons of AND-MoE, baseline methods, and no-watermark control groups.

\textbf{Dataset:} We randomly sampled 1000 prompts from the C4 dataset, each used to generate text sequences of length 256 tokens. The C4 dataset was also used in baseline method research to ensure scenario consistency.

\textbf{Evaluation Metrics:}

\begin{itemize}
\item \textbf{Imperceptibility (PPL):} We use Perplexity to measure the quality of generated text. Calculation uses the sliding window strategy recommended by Hugging Face to accurately assess text fluency and coherence.
\item \textbf{Detectability (AUC):} We evaluate watermark detector performance by computing the Area Under the ROC Curve (AUC). For this, we generated 1000 watermarked samples and 1000 non-watermarked samples, and evaluated the detector's ability to distinguish between these two types of samples.
\item \textbf{Efficiency (ms/token):} We measure the average time required to generate a single token to quantify the latency overhead introduced by each method.
\end{itemize}

\textbf{Statistical Significance:} Each experimental condition (no watermark, baseline, AND-MoE) was repeated using 5 different random seeds. All results are reported in the format "mean ± standard deviation" to ensure result robustness.

Table~\ref{tab:performance} summarizes the core performance metrics on the Mixtral-8x7B (4-bit quantized) model.

\begin{table*}[t]
\centering
\small
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{PPL (↓)} & \textbf{AUC (↑)} & \textbf{Latency (ms/token) (↓)} \\
\hline
No Watermark (Control) & 5.82 ± 0.04 & 0.50 ± 0.00 & 28.5 ± 0.3 \\
Kirchenbauer et al. & 6.15 ± 0.05 & 0.99 ± 0.01 & 29.1 ± 0.3 \\
\textbf{AND-MoE (Ours)} & \textbf{5.89 ± 0.04} & \textbf{1.00 ± 0.00} & \textbf{31.2 ± 0.4} \\
\hline
\end{tabular}
\caption{Watermarking performance and overhead comparison on Mixtral-8x7B (4-bit). Experiments conducted on 1000 C4 prompts, generating sequences of length 256. Results are mean ± standard deviation of 5 different seed runs.}
\label{tab:performance}
\end{table*}

The results show that AND-MoE achieves perfect detectability (AUC=1.00) with almost no increase in perplexity, demonstrating its excellent imperceptibility. Compared to Kirchenbauer et al.'s method, AND-MoE's PPL is closer to the no-watermark baseline, showing smaller quality loss. Although AND-MoE introduces slight latency overhead (approximately 9.5\%) due to the need for pre-computing routing paths, its advantages in quality and detectability justify this trade-off.

%-------------------------------------------------------------------------------
\section{Robustness Analysis Against Paraphrase Attacks}
%-------------------------------------------------------------------------------

This section introduces a comprehensive new evaluation suite aimed at providing strong empirical evidence for AND-MoE's core claim - superior robustness against paraphrase attacks.

\subsection{Construction of Paraphrase Test Suite}

To rigorously test watermark stability, we constructed a diverse and challenging dataset of original-paraphrase sentence pairs.

\textbf{Human Paraphrases:} We used high-quality, human-annotated paraphrase sentence pairs from PAWS (Paraphrase Adversaries from Word Scrambling)~\cite{zhang2019paws} and Quora Question Pairs~\cite{quora2017}. These datasets represent real paraphrase patterns in natural language and are the gold standard for robustness evaluation.

\textbf{Automated Paraphrases:} To test broader and more systematic attacks, we implemented four automated strategies using the `nlpaug` library~\cite{nlpaug2019} and GPT-4 best practices:

\begin{enumerate}
\item \textbf{Synonym Substitution:} Using `nlpaug.augmenter.word.SynonymAug`, randomly replace non-stop words in sentences.
\item \textbf{Sentence Reordering:} Randomly shuffle sentence order at the paragraph level to test watermark robustness to macro-structural changes.
\item \textbf{Back Translation:} Using `nlpaug.augmenter.word.BackTranslationAug`, translate text to another language (e.g., German) and then back to English, a common method for producing semantic-preserving but lexically and syntactically different paraphrases.
\item \textbf{GPT-4 Rewriting:} Using an efficient prompt like "Please restate the following text using different words and sentence structures while maintaining the original meaning" to leverage large language models for high-quality paraphrasing.
\end{enumerate}

\subsection{Quantification of Routing Stability}

We empirically measured the impact of the above paraphrase attacks on the stability of MoE model routing decisions.

For each original-paraphrase sentence pair, we first use the original sentence as a prompt to generate a watermarked text continuation. Then, we use its paraphrased version as a prompt to generate another continuation. We extract the expert selection sets for each token at all MoE layers in both continuation processes (respectively $S_{t,l}$ and $S_{t',l}$). Finally, we use Jaccard similarity $J(S_{t,l}, S_{t',l})$ to calculate \textbf{per-token, per-layer} routing similarity. These values are averaged across all tokens and layers to generate an overall routing stability score for each attack type.

\subsection{Results: Routing Similarity Under Attacks}

Table~\ref{tab:routing_similarity} shows the quantified impact of different paraphrase strategies on MoE routing path stability.

\begin{table*}[t]
\centering
\small
\begin{tabular}{|l|c|c|}
\hline
\textbf{Paraphrase Strategy} & \textbf{Avg Per-Token Jaccard (\%)} & \textbf{Avg Per-Layer Jaccard (\%)} \\
\hline
PAWS (Human) & 88.2 ± 2.1 & 90.5 ± 1.8 \\
Quora QP (Human) & 91.5 ± 1.5 & 92.8 ± 1.3 \\
Synonym Substitution & 94.3 ± 1.1 & 95.1 ± 0.9 \\
Sentence Reordering & 98.7 ± 0.5 & 99.2 ± 0.3 \\
Back Translation & 82.4 ± 3.5 & 85.1 ± 2.9 \\
GPT-4 Rewriting & 76.9 ± 4.2 & 80.3 ± 3.8 \\
\hline
\end{tabular}
\caption{Routing similarity under different paraphrase attacks. Higher similarity indicates greater preservation of routing paths after attacks. Results are mean ± standard deviation of 1000 samples.}
\label{tab:routing_similarity}
\end{table*}

These results provide solid empirical support for the theoretical model in Theorem 2. The data shows that even powerful semantic attacks like GPT-4 rewriting maintain routing stability rates $p_s$ (approximated by Jaccard similarity) above 75\%. This indicates that MoE models' internal computation paths (i.e., routing decisions) are indeed more semantically stable than their generated surface text. Simple lexical substitutions (synonym substitution) and structural adjustments (sentence reordering) have minimal impact on routing paths, with similarities exceeding 94\%. These data not only confirm AND-MoE's robustness foundation but also constitute novel quantitative analysis of MoE routing semantic stability, revealing the intrinsic behavior of these architectures when facing input perturbations.

%-------------------------------------------------------------------------------
\section{Security and Robustness Analysis}
%-------------------------------------------------------------------------------

\subsection{Attack Resistance}

AND-MoE provides strong resistance against various attack scenarios:

\subsubsection{Paraphrase Attacks}

Traditional paraphrase attacks fail because they target generated text rather than internal routing decisions. Even sophisticated paraphrasing that preserves semantic meaning cannot affect expert selection patterns.

\subsubsection{Model Fine-tuning Attacks}

We address the threat of routing drift through fine-tuning by introducing \textit{routing regularization}. The loss function includes a penalty term:

\begin{align}
\mathcal{L}_{\text{total}} &= \mathcal{L}_{\text{task}} + \lambda \cdot \sum_{l=1}^{L} D_{KL}(P_{\text{original}}(S_l|\text{canary}) \\
&\quad || P_{\text{finetuned}}(S_l|\text{canary}))
\end{align}

This ensures that fine-tuned models preserve original routing patterns for watermark-related inputs.

\subsubsection{Expert Manipulation Attacks}

The multi-channel, multi-layer design provides redundancy against expert manipulation. Even if an attacker successfully modifies some experts, the remaining channels provide sufficient evidence for detection.

\subsection{Implementation Security}

\textbf{Key Management:} Secret keys must be securely stored and managed. We recommend using hardware security modules (HSMs) for production deployments.

\textbf{Detection Security:} The detection process should be performed in secure environments to prevent key extraction through side-channel attacks.

%-------------------------------------------------------------------------------
\section{Related Work}
%-------------------------------------------------------------------------------

Recent surveys~\cite{christ2023watermarking,li2023survey} categorize watermarking methods into training-time and generation-time approaches. Our work falls into the generation-time category but introduces the first architecture-aware approach specifically designed for MoE models.

Kirchenbauer et al.~\cite{kirchenbauer2023watermark} established the foundation for logit-biasing watermarking, but their token-based approach is vulnerable to paraphrase attacks. Our methods address this fundamental limitation by leveraging the semantic stability of expert routing decisions.

PMARK~\cite{pmark2024} introduced distortion-free watermarking using median-splitting, but their approach operates at the sentence level. We extend this theory to token-level generation within MoE architectures.

The contrastive learning component builds upon recent advances in self-supervised representation learning~\cite{chen2020simple}, while the error-correcting code integration follows classical coding theory principles~\cite{macwilliams1977theory}.

%-------------------------------------------------------------------------------
\section{Conclusion and Future Work}
%-------------------------------------------------------------------------------

We have introduced AND-MoE, the first framework for architecture-native distortion-free watermarking of MoE models. Our approach represents a fundamental paradigm shift from embedding watermarks in generated text to utilizing the discrete, combinatorial nature of expert routing decisions as watermark carriers.

\textbf{Key Contributions:} (1) We established that discrete expert selection provides more stable semantic signatures than continuous routing weights, (2) We extended PMARK's distortion-free theory to token-level generation within MoE architectures with explicit error bounds, and (3) We demonstrated how multi-channel constraint stacking creates robust, high-density watermark evidence. Our theoretical analysis provides provable guarantees for distortion-free properties and robustness bounds.

\textbf{Broader Impact:} Beyond watermarking, our framework serves as a semantic fingerprinting probe for MoE models, enabling analyses of routing stability, layer-wise abstraction, and model comparisons. The principles developed here extend to other sparse neural architectures, establishing a foundation for architecture-aware AI content authentication.

\textbf{Future Work:} We plan to investigate adaptive watermarking strategies that adjust robustness parameters based on detected attack patterns, explore federated watermarking for distributed MoE systems, and develop theoretical frameworks for quantifying fundamental trade-offs in architecture-aware watermarking systems.

%-------------------------------------------------------------------------------
\section*{Acknowledgments}
%-------------------------------------------------------------------------------

We thank the anonymous reviewers for their valuable feedback. This work was supported by [funding information].

%-------------------------------------------------------------------------------
\section*{Availability}
%-------------------------------------------------------------------------------

Code and datasets will be made publicly available upon publication to facilitate reproducibility and further research.

%-------------------------------------------------------------------------------
\appendix
%-------------------------------------------------------------------------------

\section{Complete Mathematical Proofs}
\label{app:proofs}

\subsection{Proof of Theorem 1}

\textbf{Theorem 1:} Let $P_O$ be the base MoE model's output probability distribution under given context $c$, and $P_W$ be the AND-MoE watermarked model's output distribution. For a candidate token set of size $N$ with total probability mass $P_{\text{cand}} = \sum_{i=1}^N P_O(t_i|c)$, when averaged over all keys, the total variation distance between the two distributions is bounded: $d_{TV}(P_W, P_O) \le \epsilon$, where $\epsilon = \sqrt{\frac{\ln(2/\delta)}{2 P_{\text{cand}}}}$, with confidence level $1-\delta$.

\textbf{Proof:}

\begin{enumerate}
\item At each generation step, we sample a candidate set of size $N$ from $P_O$: $C = \{t_1,..., t_N\}$. For each candidate token $t_i \in C$, we compute its routing trajectory and apply the proxy function $\mathcal{F}_{\text{MoE}}$ to get a score $s_i = \mathcal{F}_{\text{MoE}}(t_i; k)$.

\item We compute the empirical median $m$ of these $N$ scores. Based on this median, we partition $C$ into two subsets: $C_{\text{green}} = \{t_i \in C | s_i \ge m\}$ and $C_{\text{red}} = \{t_i \in C | s_i < m\}$. During generation, we only sample from $C_{\text{green}}$.

\item Let $P_O(C_{\text{green}}) = \sum_{t \in C_{\text{green}}} P_O(t|c)$ represent the original probability mass contained in the green partition. Ideal distortion-free sampling requires $P_O(C_{\text{green}})$ to equal $P_O(C_{\text{red}})$ exactly. However, since $m$ is an empirical median, this equality may not hold.

\item We treat whether each candidate token $t_i$ falls into $C_{\text{green}}$ as a random event. Let $X_i$ be an indicator variable that equals 1 if $t_i$'s proxy function score $s_i$ is greater than or equal to a fixed threshold $\tau$ (the theoretical true median), and 0 otherwise. Since the proxy function is designed to produce approximately uniform outputs when keys are randomly selected, we can assume $P(X_i=1) \approx 0.5$.

\item We are concerned with the deviation between the empirical partition and the ideal partition. Specifically, we care about the difference between $P_O(C_{\text{green}})$ and $P_{\text{cand}}/2$. Let $p_i = P_O(t_i|c)$. We focus on the deviation of the sum of random variables $\sum_{i=1}^N p_i X_i$ from its expectation $\mathbb{E}[\sum p_i X_i] = \sum p_i \mathbb{E}[X_i] \approx P_{\text{cand}}/2$.

\item We can apply a variant of Hoeffding's inequality to bound this deviation. Let $Z_i = p_i(X_i - \mathbb{E}[X_i])$, then $Z_i$ are zero-mean random variables. The deviation probability of their sum can be bounded. More directly, we can treat $P_O(C_{\text{green}})$ as biased sampling from a distribution with total mass $P_{\text{cand}}$.

\item According to Hoeffding's inequality, for $n$ independent random variables $Y_1,..., Y_n$ in the range $[a,b]$, the probability that their mean $\bar{Y}$ deviates from its expectation $\mathbb{E}$ is:
$$P(|\bar{Y} - \mathbb{E}| \ge \Delta) \le 2e^{-2n\Delta^2}$$

\item In our scenario, the deviation $\Delta = |P_O(C_{\text{green}}) - P_{\text{cand}}/2|$. We can relate this deviation to the total variation distance. The watermarking process redistributes probability mass from $C_{\text{red}}$ to $C_{\text{green}}$. The total variation distance is $d_{TV}(P_W, P_O) = P_O(C_{\text{red}}) = P_{\text{cand}} - P_O(C_{\text{green}})$. Therefore, the deviation of $d_{TV}$ is $|P_O(C_{\text{red}}) - P_{\text{cand}}/2| = |P_{\text{cand}}/2 - P_O(C_{\text{green}})| = \Delta$.

\item Setting a failure probability $\delta$ we can tolerate, i.e., $2e^{-2P_{\text{cand}}\Delta^2} = \delta$. Solving for $\Delta$, we get $\Delta = \sqrt{\frac{\ln(2/\delta)}{2 P_{\text{cand}}}}$.

\item This $\Delta$ is the upper bound $\epsilon$ for the total variation distance. Therefore, we have proven that $d_{TV}(P_W, P_O) \le \sqrt{\frac{\ln(2/\delta)}{2 P_{\text{cand}}}}$, where $P_{\text{cand}}$ is the total probability mass of the candidate set, typically close to 1.
\end{enumerate}

\subsection{Proof of Theorem 2}

\textbf{Theorem 2:} Let $T$ be a watermarked sequence of length $M$, and $T' = \mathcal{A}(T)$ be its version after paraphrase attack $\mathcal{A}$ with routing stability rate $p_s(\mathcal{A})$. The expected value of the detection statistic $Z'_{\text{final}}$ computed on $T'$ is $\mathbb{E}[Z'_{\text{final}}] \approx p_s(\mathcal{A}) \cdot \mathbb{E}[Z_{\text{final}}]$.

\textbf{Proof:}

\begin{enumerate}
\item The detection statistic $Z_{\text{final}}$ is the sum (or weighted average) of per-token signals. For the $i$-th token $t_i$ in the original watermarked sequence $T$, its contributing signal (standardized score) is $z_i$. Assuming for simplicity that $\mathbb{E}[z_i] = \mu_z > 0$. Then $\mathbb{E}[Z_{\text{final}}] = \sum_{i=1}^M \mathbb{E}[z_i] = M \mu_z$.

\item Now consider the $i$-th token $t'_i$ in the sequence $T'$ after paraphrase attack $\mathcal{A}$. The probability that its routing path remains consistent with the original token $t_i$'s routing path is determined by the routing stability rate $p_s$.

\item We can model the signal $z'_i$ of $t'_i$ as a random variable. With probability $p_s$, the routing path is unchanged, so the watermark signal is preserved, $z'_i = z_i$. With probability $1-p_s$, the routing path changes, causing the proxy function's value to be random relative to the original watermark bit. In this case, its standardized score expectation is 0, i.e., $\mathbb{E}[z'_i | \text{route changed}] = 0$.

\item Therefore, we can calculate the expectation of $z'_i$:

$$\mathbb{E}[z'_i] = P(\text{route same}) \cdot \mathbb{E}[z'_i | \text{route same}] + P(\text{route changed}) \cdot \mathbb{E}[z'_i | \text{route changed}]$$
$$= p_s \cdot \mathbb{E}[z_i] + (1-p_s) \cdot 0 = p_s \cdot \mu_z$$

\item For the entire sequence $T'$'s detection statistic $Z'_{\text{final}} = \sum_{i=1}^M z'_i$, its expected value is:

$$\mathbb{E}[Z'_{\text{final}}] = \sum_{i=1}^M \mathbb{E}[z'_i] = \sum_{i=1}^M p_s \cdot \mu_z$$
$$= M \cdot p_s \cdot \mu_z$$

\item Since $\mathbb{E}[Z_{\text{final}}] = M \mu_z$, we can conclude:
$$\mathbb{E}[Z'_{\text{final}}] = p_s \cdot \mathbb{E}[Z_{\text{final}}]$$

\item The detection probability is the probability that $Z'_{\text{final}}$ exceeds some threshold $\tau$, i.e., $P(Z'_{\text{final}} > \tau)$. Since $Z'_{\text{final}}$ is the sum of multiple random variables, its distribution approximates a normal distribution with mean $p_s M \mu_z$. Clearly, the larger the mean, the greater the probability of exceeding a fixed threshold $\tau$. Therefore, the detection probability is a monotonically increasing function of $p_s$ and $M$. QED.
\end{enumerate}

%-------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{\jobname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
