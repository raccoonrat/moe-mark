

# **针对混合专家模型的鲁棒语义水印：从专利到顶会论文的深度分析与方法论升维**

## **1\. 对现有专利方案（RW-LSH）的批判性解构**

为了构建一篇具有顶会影响力的论文，首先必须对作为起点的专利技术方案进行一次深刻且批判性的审视。该专利提出了一种基于混合专家（Mixture-of-Experts, MoE）模型内部路由权重（Routing Weights, RW）并结合局部敏感哈希（Locality-Sensitive Hashing, LSH）的语义水印系统 1。此方案的初衷极具前瞻性——利用模型内部、零额外成本的信号来抵抗日益严峻的释义攻击（paraphrase attacks）2。然而，对其核心机制、学术定位和理论根基的深入剖析，揭示了其在通往顶级学术殿堂之路上所面临的根本性障碍。

### **1.1. 专利系统的核心机制**

该专利的核心思想是，MoE模型中的路由权重向量本身就是对当前上下文语义的一种高效、免费的表征 1。利用这一表征，可以生成一个与语义绑定的、在释义攻击下保持稳定的水印信号。其算法流程可分解为以下几个关键步骤：

1. **路由权重提取（RW Extraction）**：在生成每个新词元（token）时，系统首先将当前已生成的上下文序列输入到MoE模型中。随后，从模型的所有MoE层中提取为最后一个词元计算出的路由权重。这些来自不同层的权重向量被拼接（concatenate）成一个单一的高维向量 $R\_i$，作为当前上下文的语义表征 1。  
2. **语义哈希（Semantic Hashing）**：为了将连续且可能不稳定的高维向量 $R\_i$ 转换为一个离散、稳定的二进制签名（binary signature），系统采用LSH技术。一个由多个随机投影向量组成的秘密密钥 $K\_{lsh}$ 被用于对 $R\_i$ 进行哈希运算。具体而言，通过计算 $R\_i$ 与 $K\_{lsh}$ 中每个投影向量的点积的符号，生成一个L位的二进制签名 $S\_i$ 1。LSH的设计初衷是让相似的输入以高概率产生相同的哈希输出，这正是该方案抵抗释义攻击的理论基石。  
3. **词汇表划分（Vocabulary Partitioning）**：生成的L位签名 $S\_i$ 被用来动态地构建一个“绿色名单”（green list），即当前步骤下被鼓励生成的词元集合。该专利设计了一种新颖的“按位贡献”（per-bit contribution）机制：签名 $S\_i$ 的每一位都与一个预先生成好的、秘密的词汇池 $Pool\_j$ 相对应。如果签名的第 $j$ 位为1，则其对应的词汇池 $Pool\_j$ 就被并入当前的绿色名单中。这一“密钥-词汇池”的耦合结构是该专利配置文件的核心 1。  
4. **对数概率调整（Logit Modification）**：在模型完成原始的下一个词元概率计算（即生成logits向量）后，系统中所有位于绿色名单内的词元的logit值都会被增加一个预设的水印强度 $\\delta$。这一操作会轻微地“偏爱”绿色名单中的词元，从而在不显著影响文本质量的前提下，将水印信号嵌入到生成过程中。这种方法与学术界广泛研究的“红绿名单”或“logit-biasing”水印框架一脉相承 4。  
5. **水印检测（Detection）**：检测过程是嵌入过程的逆向重构。检测方需拥有完全相同的MoE模型和水印密钥。对于待检测文本中的每一个词元，检测器利用其前面的上下文，重新执行上述的RW提取、LSH哈希和绿色名单生成步骤，以重构出当时应该使用的绿色名单。然后，统计实际生成的词元“命中”绿色名单的频率，并通过假设检验（如Z检验）来判断该频率是否显著高于随机期望，从而判定文本是否包含水印 1。

### **1.2. 在LLM水印技术谱系中的定位**

根据对现有LLM水印技术的综述文献分析 4，该专利方案可以被精确地归类为一种**免训练（training-free）**、**生成时（generation-time）**、**基于对数概率调整（logits-biasing）** 的水印技术。

* **免训练**：该方案无需对大型语言模型本身进行任何的微调或重新训练，这极大地降低了部署成本和复杂性，是其相较于训练时水印方案的核心优势 9。  
* **生成时**：水印的嵌入发生在文本生成的解码阶段，实时地对每个词元的选择施加影响。  
* **基于对数概率调整**：其嵌入机制属于目前最主流的技术路线之一，与Kirchenbauer等人的开创性工作在框架上具有高度相似性 7。

该方案宣称的主要创新点和优势在于其**对释义攻击的鲁棒性**。传统的基于前一词元哈希的“红绿名单”方法，在面对同义词替换、句式重组等攻击时会完全失效，因为这些攻击改变了作为哈希输入的特定词元。而该专利方案试图通过锚定于更深层次的“语义”——即路由权重——来克服这一缺陷。理论上，只要释义攻击不改变文本的核心语义，路由权重向量就应保持相对稳定，从而LSH签名也能保持不变，保证水印的可检测性。这直面了当前水印技术领域最核心的挑战之一 2。

### **1.3. 根本性的理论与方法论缺陷**

尽管该专利的动机和高层设计颇具巧思，但其核心技术选择——将通用的LSH算法直接应用于拼接后的RW向量——暴露了其在理论深度和方法论严谨性上的根本不足。这些缺陷使其更像一个工程上的“巧妙拼接”，而非揭示了新科学原理的“根本性创新”，这正是其难以被顶级会议所接受的关键原因 1。

* 理论缺陷一：LSH作为通用工具与MoE特性的失配  
  LSH是一种为解决高维空间中近似最近邻搜索而设计的通用算法。其概率保证，即“在高维空间中距离相近的向量有很大概率映射到同一个哈希桶”，是建立在特定的数学假设之上的，例如向量间的距离度量（如欧氏距离或余弦相似度）以及数据的分布（如点均匀分布在超球面上）1。然而，MoE模型的路由权重向量并非一个理想的、均匀的嵌入空间。RW向量通常是稀疏的（大部分专家权重接近于零）、受top-k门控机制的硬性约束、并经过Softmax等非线性函数的处理，其分布形态复杂且不规则 13。将LSH直接应用于这样的数据，其理论保证会变得非常松散，鲁棒性在实践中也可能变得脆弱。换言之，该方案所获得的任何鲁棒性，更多地源于LSH算法本身的数学性质，而未能证明或利用MoE路由机制所特有的、可能更强的统计结构或语义鲁棒性 1。  
* 方法论缺陷二：对核心语义信号的错误识别  
  该专利方案的一个核心前提假设是：拼接后的高维RW向量是稳定且可靠的语义表征。然而，这一假设是值得商榷的。MoE架构的精髓在于稀疏激活和专家选择，其语义信息更多地体现在\*\*“哪些专家被激活”这一离散的、组合性的决策中，而非激活它们的具体权重值（浮点数）13。一个微小的、语义上无影响的输入扰动，可能不会改变被选中的专家集合（例如，top-2专家仍然是A和B），但却可能轻易地改变它们的相对得分（例如，从A:0.6, B:0.3变为A:0.5, B:0.4）。这种权重的波动对于拼接后的高维向量来说是显著的噪声，足以导致LSH签名的比特位翻转，从而破坏水印。  
  因此，该专利犯下了一个根本性的错误：它将注意力和技术手段都集中在一个连续、嘈杂且不稳定的代理信号（RW向量）上，却忽略了旁边一个离散、稳定且更能体现MoE架构本质的真实信号（被选中的专家索引集合）\*\*。一个顶会级别的贡献，应当能够识别并利用后者，而不是满足于对前者的简单处理。  
* 安全模型缺陷三：隐式且脆弱的安全性  
  该专利的安全性依赖于密钥 Klsh​ 和 Kpools​ 的保密性。然而，LSH算法本身是公开的。如果攻击者了解这一机制（灰盒攻击场景），他们可以不必进行语义上的释义攻击，而是直接在RW向量空间中进行微小的对抗性扰动，以最高效的方式寻找能够导致LSH签名比特翻转的方向 1。由于LSH的投影是线性的，这种攻击的难度可能远低于基于模型结构的、更复杂的编码方案。该专利没有提供任何关于对抗性扰动下的安全性分析，这在现代机器学习安全领域的顶会论文中是不可或缺的。

综上所述，该专利方案虽然在工程上具有一定的创新性，但其理论基础薄弱，未能深刻地利用MoE模型的结构特性，导致其鲁棒性和安全性存在明显的理论短板。这为我们提供了一个绝佳的切入点：保留其“利用MoE内部信号”的优秀动机，但彻底革新其将信号转化为水印的方法论。

## **2\. 一个基于MoE原生特性的 principled 语义签名框架**

为了将专利中的初步构想提升至顶会论文的高度，我们必须提出一个全新的、具有坚实理论基础的框架。这个框架的核心思想是：**放弃将MoE内部状态视为通用连续向量的范式，转向设计直接利用MoE架构原生离散与组合结构进行编码的水印机制。** 这不仅是一个技术上的改进，更是一种方法论上的升维，它将使我们的工作与MoE的本质紧密耦合，从而产生更深刻的科学贡献。

### **2.1. 核心论点：从连续哈希到离散组合编码**

我们提出的核心论点是，应当彻底摒弃对连续值RW向量进行哈希的思路。取而代之，我们将MoE路由器在每一层的输出视为一个**离散信号**：即被激活的top-k个专家的**索引集合**。

这种方法之所以被称为“MoE原生”（MoE-native），是因为它直接利用了MoE模型区别于密集模型的根本架构特征：通过专家选择实现的稀疏条件计算 13。我们不再将MoE层看作一个产生浮点数向量的黑箱，而是将其视为一个进行组合选择的决策模块。

我们的核心假设是：**这个离散的、组合性的信号，在面对语义扰动时，本质上比连续的RW向量更为鲁棒。** 一个释义（paraphrase）操作，虽然会引起模型内部激活值的普遍波动，但它改变所需专家知识集合（即top-k专家组）的可能性，要远小于它引起这些专家得分细微变化的可能性。例如，讨论“金融”和“经济”两个高度相关但略有不同的主题，可能会激活大部分相同的专家（如与经济、社会、数据分析相关的专家），但分配给这些专家的具体权重会有所不同。我们的框架正是要捕捉这种集合层面的稳定性。

### **2.2. MoE原生方法的优势**

采用这种MoE原生框架，将为我们的水印系统带来多方面的、根本性的优势：

* **更强的鲁棒性**：通过对专家索引集合进行编码，水印系统天然地对那些不改变top-k选择的路由得分扰动免疫。这直接解决了RW-LSH方案的核心脆弱点。攻击者必须施加足够大的扰动，才能真正地“踢出”一个专家并“换入”另一个，攻击的难度显著增加。  
* **更丰富的表达能力**：专家选择的组合空间是巨大的。对于一个拥有 $E$ 个专家的层，选择 $k$ 个专家的组合数是 $C(E, k)$。这个庞大的离散空间为我们设计水印签名提供了极大的灵活性，可以用于实现更高的信息容量（payload capacity）或构建更强大的纠错码，以抵御更强的攻击 1。  
* **更坚实的理论基础**：这一转变使我们能够将分析的数学工具从LSH的概率论和高维几何，转向**代数编码理论**和**离散数学**的更严谨领域。这使得我们可以给出确定性的、可证明的鲁棒性界限（例如，“能够纠正至多 $t$ 个专家替换的攻击”），而不是LSH所提供的松散概率保证。这种理论上的严谨性对于顶级会议的审稿人极具吸引力。

更进一步地，这项工作可以被置于一个更广阔的学术视野中。它不仅仅是提出了一种更好的水印技术，更是在探索一种**从大规模模型的离散计算路径中提取稳定、可验证信号的新方法论**。这暗示着，对于那些具有离散计算分支的现代神经网络架构（如MoE、Switch Transformers等），其计算“轨迹”本身就是一种比决定该轨迹的连续值更鲁棒的特征。

这一思路的延伸具有深远意义。首先，它将水印问题从一个单纯的安全应用，提升到了理解和验证模型内部行为的基础研究层面，这与当前AI安全与对齐领域的核心议题高度相关 2。其次，通过证明这种离散的“计算追溯”（computational trace）可以被稳健地编码，我们为其他应用打开了大门，例如模型指纹（model fingerprinting）、细粒度的模型行为调试、或检测模型内部的异常计算路径。最终，这使得论文的贡献从一个狭窄的水印解决方案，升华为一种在稀疏模型中实现“计算证明”（proof of computation）的基础性技术，这无疑将极大地提升其学术价值和影响力。

## **3\. 方法一：结合纠错码的组合式专家签名（CES）**

作为MoE原生框架的第一个具体实现，我们提出“组合式专家签名”（Combinatorial Expert Signatures, CES）方法。该方法直接将top-k专家选择的组合信息进行编码，并创新性地引入了纠错码（Error-Correcting Codes, ECC）理论，以实现对专家选择错误的代数级鲁棒性。

### **3.1. 组合式签名的形式化定义**

CES的核心在于将每一步生成时的专家选择行为，即一个 $k$ 元子集，映射为一个二进制签名。

设一个MoE层共有 E 个专家，索引为 {1,2,...,E}。对于给定的输入上下文 x，路由器网络计算出一个得分向量 R(x)∈RE。通过取分数最高的 k 个得分，我们得到一个无序的专家索引集合：

$$TopK(x) \= \\{i\_1, i\_2,..., i\_k\\} \\subset \\{1,..., E\\}$$  
接下来，我们需要一个确定性的、带密钥的编码函数 Enccomb​，它能将这个代表着“语义核心”的 k 元子集映射到一个L位的初步二进制签名 s′∈{0,1}L：

$$s' \= Enc\_{comb}(TopK(x))$$  
对于 $Enc\_{comb}$ 的具体实现，我们提出两种备选方案：

* 方案A：组合数索引（Combinatorial Number System Indexing）：  
  这是一种高效且无碰撞的编码方式。我们可以将所有可能的 k 元子集按照字典序进行排序。任何一个具体的子集 TopK(x) 在这个全排列中都有一个唯一的索引。例如，对于从 {1,2,3,4,5} 中选择3个专家，集合 {1,2,3} 的索引是0，{1,2,4} 的索引是1，以此类推。这个索引的二进制表示可以直接作为初步签名 s′。这种方法的优点是编码紧凑，但缺点是哪怕只改变集合中的一个元素，索引值也会发生巨大变化，即不具备局部敏感性。  
* 方案B：基于集合的哈希（Set-based Hashing）：  
  为了获得更好的局部敏感性，我们可以采用类似布隆过滤器（Bloom Filter）的机制。首先，我们定义 k 个不同的、带密钥的哈希函数 h1​,...,hk​。对于集合 TopK(x)={i1​,...,ik​} 中的每个专家索引 ij​，我们计算 hj​(ij​)，并将结果（一个整数）模 L，得到一个比特位的位置，然后将签名向量 s′ 的该位置为1。这种方法下，两个仅相差一个元素的集合，其生成的签名在汉明距离上可能也比较接近。密钥在此处可以是哈希函数的种子。

### **3.2. 引入纠错码实现鲁棒性**

CES方法的真正创新之处在于，它将释义攻击对水印的影响，精确地建模为一个**通信信道中的噪声问题**，并利用经典的纠错码理论来解决。一次成功的释义攻击，从我们的模型视角看，无非是使得检测端重构的专家集合 $TopK(x')$ 与生成端原始的 $TopK(x)$ 发生了微小的改变，例如替换了其中 $t$ 个专家。我们的目标就是让水印签名能够“纠正”这种错误。

我们在此引入纠错码的基本概念。ECC的核心是**汉明距离（Hamming distance）**，即两个等长二进制串在对应位置上比特值的不同个数 15。一个编码方案的核心是设计一个码本（codebook） $C \\subset \\{0,1\\}^L$，使得其中任意两个合法的码字（codewords）之间的汉明距离都足够大。

* **纠错理论**：一个关键的定理是，如果一个码本的最小汉明距离为 $d\_{min}$，那么这个码可以**检测**最多 $d\_{min}-1$ 个比特错误，并**纠正**最多 $t \= \\lfloor(d\_{min}-1)/2\\rfloor$ 个比特错误 15。

我们将这一理论集成到CES中。我们不再直接使用初步签名 $s'$，而是将其视为一个需要通过带噪声信道传输的“信息”，并用一个强大的ECC（如BCH码或里德-所罗门码）将其编码成一个更长的、具有纠错能力的码字 $s \\in C$。

我们的设计目标是：**当 $TopK(x)$ 集合发生至多 $t$ 个专家替换时，其对应的初步签名 $s'$ 经过ECC编码后得到的码字 $s$，与被扰动后的集合 $TopK(x')$ 对应的码字 $s\_{perturbed}$ 之间的汉明距离，应该小于码本的纠错能力。** 更具体地说，在检测端，即使我们基于被攻击后的文本重构出了一个略有偏差的 $TopK(x')$，并由此计算出错误的初步签名 $s'\_{perturbed}$ 和错误的码字 $s\_{perturbed}$，我们依然可以通过ECC的解码算法，将其唯一地、确定性地纠正回原始的正确码字 $s$。

这种方法将LSH方案中模糊的、概率性的鲁棒性，替换为了代数编码理论中精确的、可证明的鲁棒性保证。我们可以明确地宣称：“本水印系统能够抵抗任何导致不超过 $t$ 个top-k专家集合变化的释义攻击。” 这是一个远比“可能鲁棒”更具科学说服力的论断 1。

### **3.3. 数学形式化与算法流程**

下面我们给出CES+ECC方法的完整算法流程。

* **初始化/密钥生成（离线）**：  
  1. 由水印所有者定义系统参数：专家选择数 $k$，初步签名长度 $L\_{msg}$，最终码字长度 $L\_{code}$，以及期望的纠错能力 $t$。  
  2. 根据 $t$ 选择一个合适的ECC，其最小汉明距离 $d\_{min} \\ge 2t+1$。生成该ECC的生成矩阵 $G \\in \\{0,1\\}^{L\_{msg} \\times L\_{code}}$ 和校验矩阵 $H \\in \\{0,1\\}^{(L\_{code}-L\_{msg}) \\times L\_{code}}$。  
  3. 生成 $Enc\_{comb}$ 函数所需的密钥 $K\_{enc}$（例如，哈希种子）。  
  4. 生成与最终码字的每一位（共 $L\_{code}$ 位）相对应的秘密词汇池 $\\{Pool\_0,..., Pool\_{L\_{code}-1}\\}$。  
  5. 秘密密钥集合为 $K\_{CES} \= \\{G, H, K\_{enc}, \\{Pool\_j\\}\\}$。  
* **水印嵌入（在线，文本生成时）**：  
  1. 对于当前上下文 $x$，从MoE模型中提取top-k专家集合 $TopK(x)$。  
  2. 使用 $K\_{enc}$ 计算初步签名（信息）：$s' \= Enc\_{comb}(TopK(x)) \\in \\{0,1\\}^{L\_{msg}}$。  
  3. 通过生成矩阵进行ECC编码，得到最终码字（签名）：$s \= s' \\cdot G \\in \\{0,1\\}^{L\_{code}}$。（运算在伽罗瓦域 $GF(2)$ 上进行）。  
  4. 根据签名 $s$ 的每一位，构建绿色名单 $G\_i$：$G\_i \= \\bigcup\_{j: s\_j=1} Pool\_j$。  
  5. 对 $G\_i$ 中的词元进行logit增强，然后采样生成下一个词元。  
* **水印检测（在线）**：  
  1. 对于待检测文本中的词元 $t\_i$ 及其上下文 $C\_i$，重构出预期的top-k专家集合 $TopK\_{reconstructed}$。注意，由于文本可能被攻击，$TopK\_{reconstructed}$ 可能与原始集合不完全一致。  
  2. 计算出被扰动的信息：$s'\_{reconstructed} \= Enc\_{comb}(TopK\_{reconstructed})$。  
  3. 计算出被扰动的码字：$s\_{reconstructed} \= s'\_{reconstructed} \\cdot G$。  
  4. **纠错步骤**：这是与原始专利方案最关键的不同。我们并不直接使用 $s\_{reconstructed}$。相反，我们利用ECC的解码算法（例如，使用校验矩阵 $H$ 的综合解码法）来寻找码本 $C$ 中与 $s\_{reconstructed}$ 汉明距离最近的合法码字 $s\_{corrected}$。  
  5. 如果 $s\_{reconstructed}$ 与 $s\_{corrected}$ 的汉明距离小于等于 $t$，则解码成功。我们使用纠正后的码字 $s\_{corrected}$ 来构建预期的绿色名单 $G\_i$。  
  6. 判断实际词元 $t\_i$ 是否属于基于 $s\_{corrected}$ 构建的 $G\_i$。  
  7. 最后，对所有词元的命中情况进行统计裁决（如Z检验）。

通过引入ECC，CES方法建立了一道坚固的防线。只要攻击强度不足以使专家集合的变化超出ECC的纠错半径，水印信号就能被完美恢复，从而实现极高的检测鲁棒性。

## **4\. 方法二：用于分层语义编码的轨迹图哈希（TGH）**

CES方法捕捉了在单个计算步骤中（通常是某个特定MoE层）的专家选择组合，这是一种静态的语义快照。为了获取更丰富、更动态的语义信号，我们提出第二种方法：轨迹图哈希（Trajectory Graph Hashing, TGH）。TGH的核心思想是，将模型在处理单个输入时，跨越**多个MoE层**的专家选择序列视为一个“计算轨迹”，并将这个具有内在结构的轨迹作为一个整体进行编码。

### **4.1. 专家轨迹的概念**

现代的大型MoE模型，如Mixtral，通常在多个Transformer层中交替使用MoE层和密集层 16。当模型处理一个输入上下文以决定下一个词元时，信息流会依次穿过这些MoE层。在第 l 个MoE层，路由器会选择一个top-k专家集合 TopKl​(x)。我们将这一系列的选择串联起来，形成一个专家轨迹：

$$T(x) \= (TopK\_1(x), TopK\_2(x),..., TopK\_{L\_{moe}}(x))$$

其中 Lmoe​ 是模型中MoE层的总数。  
这个轨迹 $T(x)$ 蕴含了比单层专家集合更丰富的信息。它不仅告诉我们“哪些”专家被认为是相关的，还揭示了模型“如何”在不同抽象层次上逐步处理和传递语义信息。例如，底层的MoE层可能选择处理句法和词汇模式的专家，而高层的MoE层则可能激活负责更抽象概念推理的专家。这个轨迹因此构成了一个关于模型内部“思考过程”的、具有层次结构的签名。我们假设，这个完整的处理轨迹对于语义上等价的输入，其结构性变化会更小，因此是一个更稳定、更难被攻击者模仿或破坏的水印载体。

### **4.2. 从轨迹到签名：图哈希的应用**

直接对轨迹 $T(x)$ 这样一个复杂的、由集合序列构成的对象进行编码是相当困难的。传统的哈希方法（如LSH）适用于向量，而CES适用于单个集合。为了处理轨迹的结构信息，我们建议借鉴\*\*图嵌入（Graph Embedding）**和**图哈希（Graph Hashing）\*\*领域的思想 1。

我们可以将整个MoE模型的专家系统想象成一个大的分层图。图中的节点是每个层中的专家。轨迹 $T(x)$ 则可以被看作是在这个大图上形成的一个特定的子图或路径集合。我们的任务就是设计一个哈希函数 $H\_{graph}$，它能够将这个轨迹的结构特征映射为一个固定长度的二进制向量。

实现 $H\_{graph}$ 的具体技术路径包括：

* 路径特征工程（Path Feature Engineering）：  
  这是一种直接且可解释的方法。我们首先定义一个特征提取函数 Φ(⋅)，将轨迹 T(x) 转换为一个高维特征向量 v∈RD。这个向量可以包含多种统计特征，例如：  
  * **专家激活频率**：在整个轨迹中，每个专家被激活的总次数。  
  * **层间转移模式**：从第 $l$ 层的专家 $i$ “转移”到第 $l+1$ 层的专家 $j$ 的频率（可以构建一个层间转移矩阵）。  
  * 组合模式：特定专家组合（如专家A和B）在同一层内共同出现的频率。  
    一旦获得了特征向量 v=Φ(T(x))，我们就可以使用一种带密钥的投影方法（类似于LSH，但作用于我们精心设计的特征空间而非原始RW空间）将其二值化为最终的签名 s。  
* 学习式图嵌入（Learned Graph Embedding）：  
  一种更先进、更强大的方法是利用一个小型、预训练的图神经网络（Graph Neural Network, GNN）来学习轨迹的嵌入表示。我们可以将每个轨迹 T(x) 动态地构建为一个小型图，然后输入到一个固定的GNN编码器中，生成一个紧凑的嵌入向量。这个向量随后被二值化。  
  需要强调的是，这个GNN是一个独立于主LLM的、轻量级的辅助模块。它的训练可以在离线阶段完成（例如，使用与KLQ类似的对比学习方法），并且在在线嵌入水印时，它只进行一次前向传播，计算开销可控。这依然遵循了“零主模型训练成本”的原则。

### **4.3. 数学形式化与算法流程**

TGH方法的实现流程如下：

* **初始化/密钥生成（离线）**：  
  1. 定义系统参数：轨迹特征提取函数 $\\Phi$ 的具体形式，或者GNN编码器的架构。  
  2. 生成用于将特征向量/嵌入向量二值化的哈希函数 $H$ 的密钥 $K\_H$（例如，一组随机投影矩阵）。  
  3. （可选但推荐）设计一个ECC方案，用于增强最终签名的鲁棒性。  
  4. 生成与最终签名每一位相关联的秘密词汇池。  
  5. 秘密密钥集合为 $K\_{TGH} \= \\{K\_{\\Phi}, K\_H, \\text{ECC params}, \\{Pool\_j\\}\\}$，其中 $K\_{\\Phi}$ 可能是特征提取的参数或预训练的GNN权重。  
* **水印嵌入（在线，文本生成时）**：  
  1. 对于当前上下文 $x$，通过一次模型前向传播，提取所有MoE层的top-k专家集合，构成专家轨迹 $T(x)$。  
  2. 计算轨迹的特征表示：$v \= \\Phi(T(x))$。  
  3. 计算二进制签名：$s \= H(v)$。  
  4. （可选）对 $s$ 进行ECC编码，得到最终签名 $s\_{final}$。  
  5. 使用 $s\_{final}$ 构建绿色名单，并调整logits进行采样。  
* **水印检测（在线）**：  
  1. 对给定的待检测文本，逐词元重构其上下文，并重新计算预期的专家轨迹 $T\_{reconstructed}$。  
  2. 通过与嵌入时完全相同的流程，计算出预期的（可能被扰动的）签名 $s\_{reconstructed}$。  
  3. （可选）如果使用了ECC，则对 $s\_{reconstructed}$ 进行解码纠错，得到 $s\_{corrected}$。  
  4. 使用最终得到的预期签名构建绿色名单。  
  5. 进行命中判断和统计裁决。

TGH方法通过捕捉模型内部计算的动态和层次结构，将水印信号与模型的“语义处理流程”深度绑定。相比于CES的静态快照，TGH的信号来源更丰富，维度更高，理论上更难被那些只在表面进行修改的攻击所撼动。这就好比从分析一句话的词汇构成（CES），升级到分析其句法结构和逻辑层次（TGH），鲁棒性得到了质的提升。

## **5\. 方法三：基于对比学习的密钥化可学习量化器（KLQ）**

CES和TGH方法都基于一个核心假设：我们能够通过对MoE架构的先验知识，手工设计出在释义攻击下保持稳定的特征（专家组合或轨迹）。然而，真实的语义不变性可能隐藏在更复杂、非线性的模式中。为此，我们提出第三种方法，密钥化可学习量化器（Keyed Learnable Quantizer, KLQ），它采用一种数据驱动的范式，通过学习来自动发现这种不变性。

### **5.1. 可学习方法的动机**

KLQ的动机源于一个核心洞察：与其我们去猜测什么是稳定的信号，不如让模型自己从数据中“学会”什么是稳定的。我们希望找到一个函数，这个函数能将原始的、可能不稳定的RW向量，映射到一个离散的码本空间，并且这个映射本身对语义等价的输入（即释义对）具有不变性。

这里的关键挑战在于如何实现这种学习，同时严格遵守“零主模型训练成本”的原则 1。KLQ的解决方案是：训练一个非常小型的、独立于LLM的**辅助模型**——即量化器 $Q\_k$。这个量化器就像一个“观察者”，它学习如何解读主LLM产生的RW信号，而完全不干涉主LLM的任何参数。

### **5.2. KLQ的核心机制**

KLQ是一个小型的神经网络，例如一个多层感知机（MLP）。它的特殊之处在于：

1. **密钥化（Keyed）**：$Q\_k$ 的网络权重不是随机初始化，而是由一个秘密密钥 $k$（例如，一个随机种子）确定性地生成。这意味着没有密钥 $k$，任何人都无法复现或预测这个量化器函数的具体行为，保证了水印的安全性。  
2. **量化器（Quantizer）**：$Q\_k$ 的输入是原始的、连续的RW向量 $R(x)$，输出则是一个在离散码本（codebook）上的概率分布。在实际使用中，我们通常取概率最高的码字索引作为量化的结果，这个索引即可转换为二进制签名。

KLQ的核心技术在于其训练过程，我们采用\*\*对比学习（Contrastive Learning）\*\*的框架来达成目标 23。对比学习的核心思想是“拉近相似的，推开不相似的”。

* **训练数据**：我们需要一个大规模的释义对数据集。像**PAWS** 27 和**Quora Question Pairs (QQP)** 30 这样的基准数据集是理想的选择。PAWS尤其有价值，因为它包含了大量词汇重叠度高但语义不同的“对抗性”负样本。  
* **正负样本对**：对于数据集中一个句子 $x$，其任何一个释义 $x'$ 构成了**正样本对** $(x, x')$。而数据集中任何其他与 $x$ 语义不相关的句子 $y$ 则构成了**负样本对** $(x, y)$。  
* **对比损失函数**：训练的目标是调整量化器 $Q\_k$ 的参数，使其满足以下条件：  
  * 对于正样本对 $(x, x')$，它们经过主LLM和量化器处理后得到的输出 $Q\_k(R(x))$ 和 $Q\_k(R(x'))$ 应该尽可能相似（例如，它们在码本上的概率分布的交叉熵最小，或者它们选择的码字索引相同）。  
  * 对于负样本对 $(x, y)$，其输出 $Q\_k(R(x))$ 和 $Q\_k(R(y))$ 应该尽可能不相似（例如，它们在码本上的概率分布的KL散度最大）。

通过在大量的释义数据上最小化一个对比损失函数（如InfoNCE），量化器 $Q\_k$ 被迫学习到一个映射，这个映射能够“滤除”RW向量中与释义相关的噪声和变化，只保留其核心的、不变的语义信号，并将这些信号稳定地映射到同一个离散码字上。

### **5.3. 数学形式化与算法流程**

KLQ的生命周期分为离线的训练阶段和在线的嵌入/检测阶段。

* **训练阶段（离线）**：  
  1. 选择一个秘密密钥 $k$，并用它来初始化量化器网络 $Q\_k$ 的权重。  
  2. 准备一个大规模的释义数据集。  
  3. 循环遍历训练数据批次（batch）：  
     a. 对于批次中的每个句子（包括其释义），使用冻结的（frozen）主MoE LLM来计算其对应的RW向量。  
     b. 将这些RW向量输入到量化器 Qk​ 中，得到每个输入的码本概率分布。  
     c. 根据批次内的正负样本关系，构建对比损失。例如，对于一个锚点（anchor）xi​ 和其正样本 xi+​ 以及一批负样本 {xj−​}，InfoNCE损失可以形式化为：  
     $$ \\mathcal{L}\_i \= \-\\log \\frac{\\exp(\\text{sim}(Q\_k(R(x\_i)), Q\_k(R(x\_i^+)))/\\tau)}{\\exp(\\text{sim}(Q\_k(R(x\_i)), Q\_k(R(x\_i^+)))/\\tau) \+ \\sum\_j \\exp(\\text{sim}(Q\_k(R(x\_i)), Q\_k(R(x\_j^-)))/\\tau)} $$  
     其中 sim(⋅,⋅) 是相似度函数（如点积），τ 是温度超参数。  
     d. 计算损失函数的梯度，并只更新量化器 Qk​ 的参数。主LLM的参数始终保持不变。  
  4. 训练收敛后，保存带密钥的量化器 $Q\_k$。  
* **水印嵌入（在线，文本生成时）**：  
  1. 对于当前上下文 $x$，提取其RW向量 $R(x)$。  
  2. 通过训练好的量化器进行前向传播，得到码本概率分布：$p \= Q\_k(R(x))$。  
  3. 选择概率最高的码字索引：$c \= \\arg\\max(p)$。  
  4. 将码字索引 $c$ 转换为其唯一的二进制表示 $s$。  
  5. 使用签名 $s$ 构建绿色名单并调整logits。

KLQ方法引入了一种全新的水印设计范式，可以称之为\*\*“辅助学习”\*\*。它在传统的“免训练”和“需要微调”两极之间开辟了一个极具吸引力的中间地带。这种范式允许水印系统利用数据驱动学习的强大威力来提升鲁棒性，但又将学习的成本和风险完全隔离在一个微小的、独立的辅助模块中，对昂贵的主LLM实现了“零侵入”。这是一个重要的概念性贡献，它展示了一种如何在不牺牲部署便利性的前提下，显著增强水印性能的新路径。密钥 $k$ 的存在，确保了这个学习到的强大函数本身是一个秘密，从而为系统提供了坚实的安全保障。

## **6\. 严谨的理论保障：稳定性、容量与安全性分析**

一篇顶会论文的标志不仅在于其方法的创新性，更在于其理论的完备性。为了将我们的工作从一个工程解决方案提升为一个基础科学贡献，必须为每一种提出的方法提供严谨的数学分析，围绕**稳定性（鲁棒性）**、**信息容量**和**安全性**这三个核心维度展开 1。这不仅是对方法有效性的证明，更是将其与计算机科学的基础理论（编码理论、图论、统计学习理论）进行深度链接的关键。

### **6.1. 稳定性分析（鲁棒性界定）**

稳定性的目标是形式化地证明：一个对输入文本进行的、保持语义的微小扰动，只会以一个很小的、有界的概率导致水印签名的改变。

* 对于CES+ECC方法：  
  分析将是组合与代数的。我们将一次释义攻击建模为一个随机过程，该过程以一定的概率分布，导致原始的top-k专家集合 TopK(x) 中至多有 τ 个专家被替换。分析的核心是建立 τ 与我们选择的ECC的纠错能力 t 之间的关系。  
  证明思路：首先，我们需要分析 Enccomb​ 函数的性质，即一个专家的替换会引起初步签名 s′ 中多少比特位的变化。然后，利用ECC的重量分布或距离属性，我们可以推导出：当输入信息（即 s′）发生 b 个比特错误时，解码失败的概率上界。最终，我们可以给出一个确定性的鲁棒性结论：对于任何引起不超过 tmax​ 个专家替换的攻击，水印信号的恢复成功率为100%。这里的 tmax​ 是由ECC的最小距离 dmin​ 和 Enccomb​ 的性质共同决定的。  
* 对于TGH方法：  
  分析将是概率与统计的。我们可以将专家轨迹的生成过程建模为一个马尔可夫链，其中状态是专家，层级是时间。一次释义攻击可以被看作是对这个马尔可夫链的转移概率矩阵施加了一个微小的扰动。  
  证明思路：我们将利用集中不等式（如McDiarmid's inequality或Azuma's inequality）来分析。首先，我们将轨迹特征向量 Φ(T(x)) 表示为一系列随机变量（例如，每个层级的专家选择）的函数。然后，证明这个函数满足有界差分性质（bounded differences property）。基于此，我们可以推导出，当马尔可夫链的参数发生有界扰动时，输出的特征向量 Φ(T(x)) 与其期望值的偏差被高概率地限制在一个小范围内。这个范围直接决定了最终二进制签名发生比特翻转的概率上界。  
* 对于KLQ方法：  
  分析将基于统计学习理论。我们需要为学习到的量化器 Qk​ 提供泛化误差界，即证明其在未见过的释义对上的表现也能得到保证。  
  证明思路：我们将利用对比学习的理论框架。分析可以围绕量化器 Qk​ 的Rademacher复杂度或VC维展开，这些是衡量函数族复杂度的指标。通过这些指标，我们可以给出一个泛化界，它将 Qk​ 在测试集上的“释义错误率”（即为正样本对生成不同签名的概率）与其在训练集上的经验错误率、训练样本的数量以及函数族的复杂度联系起来。这个界限将从理论上说明，只要有足够多的训练数据，我们就能学到一个足够鲁棒的量化器。

### **6.2. 信息容量分析（负载能力）**

容量分析旨在量化我们的水印系统在每个生成步骤中可以嵌入多少信息。这对于需要传递元数据（如作者ID、时间戳）的应用场景至关重要。

* **形式化定义**：对于一个产生L位二进制签名的系统，其理论上的最大信息容量为每个词元 $L$ 比特。然而，为了鲁棒性（例如，通过ECC增加冗余），有效信息容量会小于 $L$。  
* **分析**：  
  * **CES+ECC**：如果初步签名（信息）长度为 $L\_{msg}$，最终码字长度为 $L\_{code}$，则有效容量为 $L\_{msg}$ 比特，冗余度为 $L\_{code} \- L\_{msg}$。容量与可纠正的错误数 $t$ 之间存在明确的理论权衡（trade-off），这可以通过编码理论中的经典界限（如Hamming bound或Gilbert-Varshamov bound）来刻画。  
  * **TGH**：容量由二值化后的签名长度 $L$ 决定。分析的重点在于，在给定的鲁棒性要求下，我们能够支持的最大 $L$ 是多少。这与轨迹特征空间 $\\Phi(T(x))$ 的内在维度和可分性有关。  
  * **KLQ**：容量由量化器输出的离散码本大小 $C$ 决定，为 $\\log\_2(C)$ 比特。码本越大，容量越高，但训练所需的样本量和模型复杂度也可能相应增加。

### **6.3. 安全性分析（对抗恢复力）**

安全性分析的目标是评估在特定的威胁模型下，攻击者移除或伪造水印的难度。

* **威胁模型**：我们采用一个标准的**灰盒（gray-box）攻击模型**。攻击者完全了解水印的算法（CES, TGH, 或 KLQ的架构），但**不知道**属于水印所有者的秘密密钥（$K\_{CES}$, $K\_{TGH}$, 或 $k$）。  
* **分析**：  
  * **CES/TGH**：安全性主要依赖于两个方面：一是编码/哈希函数中使用的密钥（如 $K\_{enc}$, $K\_H$）的密码学安全性；二是巨大的组合空间。在不知道密钥的情况下，攻击者无法预测对文本的何种修改能够精确地将一个签名映射到另一个**合法的**（但错误的）签名。攻击者的扰动在签名空间中几乎是“盲目的”。这些扰动要么因为幅度太小而被ECC纠正，要么因为幅度太大而产生一个无效的（离所有合法码字都很远的）签名，或者严重破坏文本质量。我们将论证，找到一个能“欺骗”解码器的最小扰动在计算上是不可行的。  
  * **KLQ**：安全性建立在由密钥 $k$ 初始化的量化器网络 $Q\_k$ 的**不可预测性**上。由于网络的初始化是确定性地依赖于秘密种子 $k$ 的，攻击者无法在没有 $k$ 的情况下复现这个函数。我们将分析从外部观察（即分析大量带水印的文本）来逆向工程出 $Q\_k$ 函数的难度。对于一个足够复杂的非线性函数 $Q\_k$，这在计算上是极其困难的，类似于破解一个小型但带密钥的神经网络。

通过构建这样一个由稳定性、容量和安全性三大支柱组成的理论框架，我们的论文将展示所提出的水印方案不仅仅是有效的启发式方法，而是具有可分析、可预测性能的、 principled 的工程解决方案。这正是顶级会议所追求的科学深度和严谨性。

## **7\. 用于实证验证的综合实验设计**

理论分析为方法的有效性提供了上界和保证，而全面、严谨的实证评估则是检验其在现实世界中性能的唯一标准。本节将设计一套完整的实验方案，旨在严格验证我们提出的三种MoE原生水印方法，并将它们与相关基线进行公平比较。

### **7.1. 实验设置与基线模型**

* **模型**：为保证实验的可复现性，我们将选用公开可用的MoE大语言模型。Hugging Face等模型社区提供了丰富的选项，例如**Mixtral系列模型**或**OpenMoE**等开源项目 16。选择不同规模和架构的MoE模型可以测试我们方法的泛化能力。  
* **数据集**：  
  * **文本生成与质量评估**：我们将使用一个大规模、多样化的通用文本语料库，如**C4 (Colossal Clean Crawled Corpus)**，作为生成带水印文本的基础。生成的文本将用于评估水印的不可感知性。  
  * **鲁棒性测试（释义攻击）**：这是实验的核心。我们将采用两个公认的释义基准数据集：  
    1. **PAWS (Paraphrase Adversaries from Word Scrambling)** 27：该数据集至关重要，因为它被特意设计为具有对抗性。其中的句子对具有极高的词汇重叠度，但语义可能完全不同，这对测试水印是否真正理解“语义”而非“词汇”提出了严峻的挑战。  
    2. QQP (Quora Question Pairs) 30：该数据集提供了大量来自真实世界的释义问句对，规模庞大，可以用于测试水印在真实、嘈杂的释义场景下的表现。  
       我们将使用一个强大的预训练模型（如T5或BART）在QQP上进行微调，以构建一个自动的、高质量的释义攻击模型。  
* **基线方法**：为了凸显我们方法的优越性，必须与当前领域内最强或最相关的基线进行比较：  
  1. **专利方案 (RW-LSH)**：这是我们工作的直接出发点和首要比较对象，用以证明我们的MoE原生框架的根本性改进。  
  2. **Kirchenbauer et al. (2023)** 5：这是目前引用最广泛、影响力最大的免训练、logit-biasing水印方案之一。将其作为基线，可以展示我们的MoE**专用**方法相对于**通用**方法的优势。  
  3. **无水印 (No Watermark)**：作为黄金标准，用于测量水印对原始文本质量的损害程度（即不可感知性）。

### **7.2. 评估指标**

我们将从四个维度对所有方法进行全面评估，这四个维度构成了水印技术的“不可能三角”加上效率考量：

1. **鲁棒性 (Robustness)**：衡量水印在遭受攻击后被成功检测的概率。主要指标是**ROC曲线下面积 (Area Under the ROC Curve, AUC)**。AUC为1表示完美检测，0.5表示随机猜测。同时，我们将报告在极低的**假阳性率 (False Positive Rate, FPR)** 下的**真阳性率 (True Positive Rate, TPR)**，例如在 $FPR=1\\%$ 或 $FPR=0.1\\%$ 时的TPR，这在现实应用中更具意义。  
2. **不可感知性 (Imperceptibility)**：衡量水印对生成文本质量的影响。主要指标是**困惑度 (Perplexity, PPL)**。我们将使用一个独立的、强大的第三方LLM（如GPT-4）来计算带水印文本和无水印文本的PPL。$\\Delta PPL$（PPL的增量）越小，说明水印越隐蔽，质量影响越小。  
3. **计算开销 (Overhead)**：衡量水印嵌入过程带来的额外计算成本。主要指标是**每生成一个词元的平均延迟（毫秒/词元）**。这个指标直接关系到水印方案是否能被实际部署于在线服务。  
4. **信息容量 (Capacity)**：理论上每个词元可以嵌入的**比特数（bits/token）**。

### **7.3. 实验流程与消融研究**

* **主实验流程**：  
  1. **生成**：使用每种方法（RW-LSH, CES, TGH, KLQ, Kirchenbauer et al.）和无水印基线，在C4数据集的prompt上生成一个大规模的文本语料库。  
  2. **攻击**：将生成的语料库输入到我们训练好的释义攻击模型中，生成对应的释义版本。  
  3. **检测**：在被攻击后的文本上运行每种方法对应的检测器，记录检测分数。  
  4. **评估**：根据检测分数和真实标签（是否带水印），为每种方法在攻击后的表现绘制ROC曲线，并计算AUC和TPR@FPR。同时，计算原始带水印文本的 $\\Delta PPL$ 和生成过程中的时间开销。  
* **消融研究 (Ablation Studies)**：这是理解我们方法成功关键因素的重要步骤，对于一篇顶会论文至关重要。  
  * **对于CES+ECC**：我们将改变ECC的纠错能力 $t$（通过选择不同最小距离 $d\_{min}$ 的码）和top-k中的 $k$ 值，以展示鲁棒性、容量和计算开销之间的权衡。  
  * **对于TGH**：我们将比较不同轨迹特征提取器 $\\Phi$ 的性能，例如从简单的专家频率统计到一个小型的学习式GNN，以验证捕捉更复杂结构信息的价值。  
  * **对于KLQ**：我们将分析用于训练量化器 $Q\_k$ 的释义数据量对最终鲁棒性的影响，以展示其数据效率。  
  * **通用参数**：对于所有方法，我们将系统地改变签名长度 $L$ 和水印强度 $\\delta$，绘制出性能随这些关键超参数变化的曲线，为实际应用提供指导。

### **7.4. 预期结果汇总表**

为了清晰、直观地展示我们工作的核心成果，我们将在论文中呈现一个如下格式的汇总表格。这张表格将成为支撑我们主要论点的核心证据。

| 方法 (Method) | 鲁棒性 (AUC vs. Paraphrase) | 不可感知性 (Δ PPL) | 计算开销 (ms/token) | 容量 (bits/token) |
| :---- | :---- | :---- | :---- | :---- |
| 无水印 (No Watermark) | N/A | 0.0 | 0.0 | 0 |
| Kirchenbauer et al. (2023) | (预期中等) | (预期较低) | (预期较低) | 1 |
| 专利方案 (RW-LSH) | (预期较低) | (预期较低) | (预期较低) | $L$ |
| **提议方案: CES+ECC** | **(预期高)** | **(预期较低)** | **(预期较低)** | $L\_{msg}$ |
| **提议方案: TGH** | **(预期很高)** | **(预期中等)** | **(预期中等)** | $L$ |
| **提议方案: KLQ** | **(预期很高)** | **(预期较低)** | **(预期较低)** | $\\log\_2(C)$ |

这张表格的设计旨在让审稿人一目了然地把握论文的核心贡献：我们提出的三种MoE原生方法（CES, TGH, KLQ）能够在保持与现有先进方法相当甚至更低的质量损耗和计算开销的同时，实现**显著更高**的释义攻击鲁棒性。表格中的量化数据将为这一论断提供强有力的、不可辩驳的实证支持，从而构成论文被接受的关键。

## **8\. 论文叙事构建：定位与贡献以实现顶会发表**

一篇成功的顶会论文不仅需要坚实的技术内核和详尽的实验验证，还需要一个清晰、有力且引人入胜的叙事。本节将提供关于如何构建论文叙事、定位研究贡献的战略性建议，以最大化其影响力并提升被顶级会议接收的概率。

### **8.1. 精炼核心贡献**

论文的核心叙事不应仅仅是“我们改进了一种现有的水印技术”。一个更宏大、更具吸引力的叙事应该是：

**“我们引入了一类全新的、MoE原生的语义水印算法。这类算法通过利用稀疏专家模型内在的组合与结构特性，为抵抗语义攻击提供了可证明的鲁棒性，而这正是现有通用方法失败之处。”**

这个叙事强调了几个关键点：

* **范式转移**：明确指出工作中的概念性飞跃——从将MoE内部状态视为通用向量空间，到利用其独特的、离散的计算结构。这是最核心的智力创新。  
* **架构感知**：强调“MoE原生”或“架构感知”（architecture-aware）的设计理念，这顺应了当前机器学习领域从通用模型向专用、高效架构演进的趋势。  
* **可证明的鲁棒性**：突出理论分析的成果，将鲁棒性从一个经验性的观察提升到一个具有数学保障的属性。

### **8.2. 突出顶会论文的三大支柱**

整篇论文的结构和内容都应围绕以下三个支柱来组织，以展现其全面性和深度：

1. **方法论的创新性 (Novel Methodology)**：清晰地呈现CES、TGH和KLQ作为MoE原生设计理念下的三种不同但互补的实现。应强调它们各自的特点：CES是**代数和组合的**，TGH是**结构和图论的**，而KLQ是**数据驱动和学习的**。这表明我们对这个新的设计空间进行了全面而深入的探索，而非浅尝辄止。  
2. **理论的深度 (Theoretical Depth)**：将第六节的理论分析作为一个独立且重要的部分突出展示。稳定性界定、容量分析和安全性论证，共同构成了将本研究与纯粹的经验性工作区分开来的科学严谨性基石。在引言和结论中都应反复强调这些理论贡献。  
3. **实证的严谨性 (Rigorous Empirical Validation)**：第七节的综合实验设计是理论优势转化为现实性能的最终证明。应特别强调实验的严谨性，包括：使用了具有挑战性的**对抗性基准（PAWS）**，对比了**强大的基线（Kirchenbauer et al.）**，并进行了**详尽的消融研究**。

### **8.3. 回答“为何是现在？” (Why Now?)**

论文的引言部分必须有力地回答这个问题，以建立研究的紧迫性和必要性。可以通过强调两个并行发展的趋势来构建动机：

1. **MoE模型的崛起与普及**：随着Mixtral等模型的成功，MoE架构已从学术概念走向主流应用，成为构建下一代超大规模语言模型的关键技术 13。因此，为这类模型设计专用的、高效的安全工具变得至关重要。  
2. **水印攻击技术的演进**：简单的文本编辑攻击已不再是主要威胁，基于模型的、能够保持语义一致性的释义攻击正变得越来越强大和普遍，这使得现有的大部分水印技术变得脆弱不堪 2。

我们的工作恰好处于这两个趋势的交汇点，是应对新挑战（高级攻击）和新机遇（新模型架构）的及时且必要的解决方案。

### **8.4. 论文结构对影响力的最大化**

* **摘要 (Abstract)**：开篇即直面核心问题（语义攻击破坏了现有水印），并立即抛出核心思想（利用MoE原生的组合签名）。用一句话总结三种方法的提出以及理论和实验上的显著优势。  
* **引言 (Introduction)**：讲述一个清晰的故事。从LLM滥用的普遍问题开始，过渡到水印技术的重要性，然后指出当前技术的脆弱性，特别是现有方法（如专利方案）与MoE架构之间的理论失配。最后，自然地引出我们的MoE原生框架作为解决方案，并概述论文的三大贡献（方法、理论、实验）。  
* **方法 (Methods)**：为CES、TGH和KLQ分别设立清晰的子章节。每个子章节都应包含动机、核心机制和算法流程，并配以图示。  
* **理论与实验 (Theory and Experiments)**：将理论分析和实验验证作为两个独立、同等重要的章节。理论部分展示数学推导和最终的定理/界限；实验部分则以汇总结果表为核心，辅以详细的图表和分析。  
* **结论 (Conclusion)**：再次重申核心贡献——开创了架构感知的水印设计新范式。同时，可以展望该思想的更广泛应用，例如在模型可解释性、指纹识别和计算完整性验证等领域，从而提升论文的长期影响力。

通过精心构建这样的叙事和结构，这篇论文将不仅仅是对一个专利的改进，而是一项定义了新问题、提出了新范式、并提供了坚实理论与实践基础的、有望在顶级会议上产生深远影响的开创性工作。

#### **Works cited**

1. 专利\_一种鲁棒语义水印系统及其配置方法.pdf  
2. Watermarking Techniques for Large Language Models: A Survey \- arXiv, accessed October 20, 2025, [https://arxiv.org/html/2409.00089v1](https://arxiv.org/html/2409.00089v1)  
3. Ensemble Watermarks for Large Language Models \- ACL Anthology, accessed October 20, 2025, [https://aclanthology.org/2025.acl-long.145.pdf](https://aclanthology.org/2025.acl-long.145.pdf)  
4. Watermarking for Large Language Models: A Survey \- ResearchGate, accessed October 20, 2025, [https://www.researchgate.net/publication/391257786\_Watermarking\_for\_Large\_Language\_Models\_A\_Survey](https://www.researchgate.net/publication/391257786_Watermarking_for_Large_Language_Models_A_Survey)  
5. A Watermark for Large Language Models \- arXiv, accessed October 20, 2025, [https://arxiv.org/pdf/2301.10226](https://arxiv.org/pdf/2301.10226)  
6. A Watermark for Large Language Models \- TL;DR With Paper Author \- Arize AI, accessed October 20, 2025, [https://arize.com/blog/a-watermark-for-large-language-models/](https://arize.com/blog/a-watermark-for-large-language-models/)  
7. \[2301.10226\] A Watermark for Large Language Models \- arXiv, accessed October 20, 2025, [https://arxiv.org/abs/2301.10226](https://arxiv.org/abs/2301.10226)  
8. A Watermark for Large Language Models \- Proceedings of Machine Learning Research, accessed October 20, 2025, [https://proceedings.mlr.press/v202/kirchenbauer23a.html](https://proceedings.mlr.press/v202/kirchenbauer23a.html)  
9. Watermarking for Large Language Models: A Survey \- MDPI, accessed October 20, 2025, [https://www.mdpi.com/2227-7390/13/9/1420](https://www.mdpi.com/2227-7390/13/9/1420)  
10. A Survey of Text Watermarking in the Era of Large Language Models \- arXiv, accessed October 20, 2025, [https://arxiv.org/html/2312.07913v4](https://arxiv.org/html/2312.07913v4)  
11. Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data? \- AAAI Publications, accessed October 20, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/34684/36839](https://ojs.aaai.org/index.php/AAAI/article/view/34684/36839)  
12. On the Reliability of Watermarks for Large Language Models \- OpenReview, accessed October 20, 2025, [https://openreview.net/forum?id=DEJIDCmWOz](https://openreview.net/forum?id=DEJIDCmWOz)  
13. Mixture of Experts in LLMs \- Al-banna Tutorials, accessed October 20, 2025, [https://albanna-tutorials.com/moe.html](https://albanna-tutorials.com/moe.html)  
14. Mixture of Experts Explained \- Hugging Face, accessed October 20, 2025, [https://huggingface.co/blog/moe](https://huggingface.co/blog/moe)  
15. Error correcting codes | Brilliant Math & Science Wiki, accessed October 20, 2025, [https://brilliant.org/wiki/error-correcting-codes/](https://brilliant.org/wiki/error-correcting-codes/)  
16. Top 10 open source LLMs for 2025 \- Instaclustr, accessed October 20, 2025, [https://www.instaclustr.com/education/open-source-ai/top-10-open-source-llms-for-2025/](https://www.instaclustr.com/education/open-source-ai/top-10-open-source-llms-for-2025/)  
17. An introduction to graph embeddings \- Linkurious, accessed October 20, 2025, [https://linkurious.com/graph-embeddings/](https://linkurious.com/graph-embeddings/)  
18. What is Graph Embedding? A Practical Guide for Developers, accessed October 20, 2025, [https://www.puppygraph.com/blog/graph-embedding](https://www.puppygraph.com/blog/graph-embedding)  
19. Scalable Graph Embedding Enhanced by Content-Preserving Locality Sensitive Hashing \- Xiusi Chen, accessed October 20, 2025, [https://xiusic.github.io/papers/aaai18.pdf](https://xiusic.github.io/papers/aaai18.pdf)  
20. Understanding Graph Embedding Methods and Their Applications | SIAM Review, accessed October 20, 2025, [https://epubs.siam.org/doi/10.1137/20M1386062](https://epubs.siam.org/doi/10.1137/20M1386062)  
21. Hashing with Graphs \- Google Research, accessed October 20, 2025, [https://research.google.com/pubs/archive/37599.pdf](https://research.google.com/pubs/archive/37599.pdf)  
22. Hashing-based semantic relevance attributed knowledge graph embedding enhancement for deep probabilistic recommendation \- PubMed Central, accessed October 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9075930/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9075930/)  
23. Unsupervised Hashing with Contrastive Information Bottleneck \- IJCAI, accessed October 20, 2025, [https://www.ijcai.org/proceedings/2021/0133.pdf](https://www.ijcai.org/proceedings/2021/0133.pdf)  
24. Contrastive Learning: A Comprehensive Guide | by Juan C Olamendy | Medium, accessed October 20, 2025, [https://medium.com/@juanc.olamendy/contrastive-learning-a-comprehensive-guide-69bf23ca6b77](https://medium.com/@juanc.olamendy/contrastive-learning-a-comprehensive-guide-69bf23ca6b77)  
25. Contrastive Learning with SimCLR | Deep Learning Animated \- YouTube, accessed October 20, 2025, [https://www.youtube.com/watch?v=UqJauYELn6c](https://www.youtube.com/watch?v=UqJauYELn6c)  
26. Contrastive Loss from Scratch \- deep learning \- Stack Overflow, accessed October 20, 2025, [https://stackoverflow.com/questions/79279472/contrastive-loss-from-scratch](https://stackoverflow.com/questions/79279472/contrastive-loss-from-scratch)  
27. PAWS: Paraphrase Adversaries from Word ... \- ACL Anthology, accessed October 20, 2025, [https://aclanthology.org/N19-1131.pdf](https://aclanthology.org/N19-1131.pdf)  
28. PAWS (Paraphrase Word Scrambling) \- Kaggle, accessed October 20, 2025, [https://www.kaggle.com/datasets/thedevastator/the-paws-dataset-for-paraphrase-identification](https://www.kaggle.com/datasets/thedevastator/the-paws-dataset-for-paraphrase-identification)  
29. google-research-datasets/paws: This dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. \- GitHub, accessed October 20, 2025, [https://github.com/google-research-datasets/paws](https://github.com/google-research-datasets/paws)  
30. Another Look At Paraphrase Identification \- OpenReview, accessed October 20, 2025, [https://openreview.net/pdf?id=jj3kSvsyDI](https://openreview.net/pdf?id=jj3kSvsyDI)  
31. MOE/Mixture of Experts Models (see also "source" cll) \- a DavidAU Collection, accessed October 20, 2025, [https://huggingface.co/collections/DavidAU/moe-mixture-of-experts-models-see-also-source-cll-67579e54e1a2dd778050b928](https://huggingface.co/collections/DavidAU/moe-mixture-of-experts-models-see-also-source-cll-67579e54e1a2dd778050b928)  
32. Daily Papers \- Hugging Face, accessed October 20, 2025, [https://huggingface.co/papers?q=Mixture%20of%20Experts%20(MoE)%20LLMs](https://huggingface.co/papers?q=Mixture+of+Experts+\(MoE\)+LLMs)  
33. ibm-research/MoLM-700M-8B \- Hugging Face, accessed October 20, 2025, [https://huggingface.co/ibm-research/MoLM-700M-8B](https://huggingface.co/ibm-research/MoLM-700M-8B)  
34. XueFuzhao/OpenMoE: A family of open-sourced Mixture-of-Experts (MoE) Large Language Models \- GitHub, accessed October 20, 2025, [https://github.com/XueFuzhao/OpenMoE](https://github.com/XueFuzhao/OpenMoE)